{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_VlmIivglEk0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model architecture:**\n",
        "\n",
        "Creating a sequential model with three encoder layers and three decoder layers.\n",
        "\n",
        "The input layer takes a flattened MNIST image of size 784 (28x28 pixels) as input. The encoder layers use the 'relu' activation function, while the decoder layers use the 'sigmoid' activation function. We compile the autoencoder using the 'adam' optimizer and the 'binary_crossentropy' loss function."
      ],
      "metadata": {
        "id": "CzFwbdSKvH2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Specifying only the architecture of the encoder\n",
        "model=Sequential([                                                      #dense means fully connected layer in ANN\n",
        "                  #Encoder layers\n",
        "                  Dense(units=128, input_shape=(784,), activation='relu'), # 1 encoder layer with 128 nodes with relu activation, input shape is an 784\n",
        "                  Dense(units=64, activation='relu'),                   # 2nd encoder layer with 64 nodes with relu activation\n",
        "                  Dense(units=32, activation='relu', name='encoder_output'),                  # 3rd encoder layer with 32 \n",
        "                  #Decoder layers\n",
        "                  Dense(units=64, activation='relu'),                    # 1st decoder layer with 64 nodes with relu activation,\n",
        "                  Dense(units=128, activation='relu'),                   # 2nd decoder layer with 128 nodes with relu activation\n",
        "                  Dense(units=784, activation='sigmoid')                  # Output layer with 784 nodes with sigmoid activation \n",
        "                  \n",
        "])"
      ],
      "metadata": {
        "id": "aaYkU1I_mJep"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why not using softmax at the output layer?**\n",
        "\n",
        "In the context of autoencoders, the activation function used in the output layer depends on the type of data that the autoencoder is trained on.\n",
        "\n",
        "The sigmoid activation function is commonly used for binary classification problems, where the output is a single binary value (0 or 1). It maps the input value to a range between 0 and 1, which can be interpreted as the probability of the input belonging to the positive class.\n",
        "\n",
        "In the case of autoencoders, the input images are grayscale images with pixel values normalized between 0 and 1. Since the output of the autoencoder is also an image with pixel values between 0 and 1, the sigmoid activation function is a good choice for the output layer.\n",
        "\n",
        "On the other hand, the softmax activation function is commonly used for multi-class classification problems, where the output is a probability distribution over multiple classes. It maps the input value to a range between 0 and 1 and normalizes the output so that the sum of all output values equals 1.\n",
        "\n",
        "Therefore, in the context of autoencoders for image reconstruction, the sigmoid activation function is usually used in the output layer, while softmax is more suitable for classification problems."
      ],
      "metadata": {
        "id": "zpF-4fstGKY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XT0xBrFqoJGd",
        "outputId": "d6f18643-1e5a-49f9-df09-af82370f3563"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_33 (Dense)            (None, 128)               100480    \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " encoder_output (Dense)      (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_35 (Dense)            (None, 64)                2112      \n",
            "                                                                 \n",
            " dense_36 (Dense)            (None, 128)               8320      \n",
            "                                                                 \n",
            " dense_37 (Dense)            (None, 784)               101136    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 222,384\n",
            "Trainable params: 222,384\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss function in the autoencoders:**\n",
        "\n",
        "Autoencoders are typically used for dimensionality reduction and reconstruction of input data, not for classification. The output of the autoencoder is a reconstructed version of the input, and the quality of the reconstruction is evaluated using a loss metric such as mean squared error (MSE) or binary cross-entropy (BCE). The lower the value of the loss metric, the better the reconstruction quality. In the code I provided, the loss metric used is MSE, and the training and validation loss values are reported during training.\n",
        "\n",
        "Next, we load the MNIST dataset and normalize the data. We then train the autoencoder using the 'fit' method on the training data, with a batch"
      ],
      "metadata": {
        "id": "FqOEG0onr_0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compile the autoencoder\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy')"
      ],
      "metadata": {
        "id": "lEN52mX4oRWx"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading, preprocessing and splitting the MNIST dataset**"
      ],
      "metadata": {
        "id": "DYy9K87yoiX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the MNIST dataset\n",
        "(x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# normalize the data and flatten the images\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))"
      ],
      "metadata": {
        "id": "bs_KhgSbobBO"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training the model with the preprocessed data**"
      ],
      "metadata": {
        "id": "_q6WtPVpoqsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train the autoencoder\n",
        "model.fit(x_train, x_train,epochs=50,\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1A5_jF8qouan",
        "outputId": "753bb2b6-c02b-4fd4-8624-792e4cd1c4c7"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "235/235 [==============================] - 7s 24ms/step - loss: 0.2534 - val_loss: 0.1710\n",
            "Epoch 2/50\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.1518 - val_loss: 0.1379\n",
            "Epoch 3/50\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.1328 - val_loss: 0.1256\n",
            "Epoch 4/50\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.1238 - val_loss: 0.1189\n",
            "Epoch 5/50\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.1182 - val_loss: 0.1144\n",
            "Epoch 6/50\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.1137 - val_loss: 0.1108\n",
            "Epoch 7/50\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 0.1103 - val_loss: 0.1079\n",
            "Epoch 8/50\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.1078 - val_loss: 0.1055\n",
            "Epoch 9/50\n",
            "235/235 [==============================] - 5s 23ms/step - loss: 0.1057 - val_loss: 0.1036\n",
            "Epoch 10/50\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.1037 - val_loss: 0.1018\n",
            "Epoch 11/50\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.1018 - val_loss: 0.0998\n",
            "Epoch 12/50\n",
            "235/235 [==============================] - 5s 23ms/step - loss: 0.1001 - val_loss: 0.0983\n",
            "Epoch 13/50\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.0986 - val_loss: 0.0972\n",
            "Epoch 14/50\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 0.0975 - val_loss: 0.0960\n",
            "Epoch 15/50\n",
            "235/235 [==============================] - 5s 23ms/step - loss: 0.0964 - val_loss: 0.0951\n",
            "Epoch 16/50\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.0956 - val_loss: 0.0941\n",
            "Epoch 17/50\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0948 - val_loss: 0.0940\n",
            "Epoch 18/50\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0942 - val_loss: 0.0929\n",
            "Epoch 19/50\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.0935 - val_loss: 0.0923\n",
            "Epoch 20/50\n",
            "235/235 [==============================] - 5s 23ms/step - loss: 0.0929 - val_loss: 0.0916\n",
            "Epoch 21/50\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.0923 - val_loss: 0.0913\n",
            "Epoch 22/50\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.0918 - val_loss: 0.0909\n",
            "Epoch 23/50\n",
            "235/235 [==============================] - 5s 23ms/step - loss: 0.0913 - val_loss: 0.0906\n",
            "Epoch 24/50\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.0909 - val_loss: 0.0899\n",
            "Epoch 25/50\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 0.0905 - val_loss: 0.0894\n",
            "Epoch 26/50\n",
            "235/235 [==============================] - 5s 23ms/step - loss: 0.0901 - val_loss: 0.0892\n",
            "Epoch 27/50\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.0898 - val_loss: 0.0892\n",
            "Epoch 28/50\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0895 - val_loss: 0.0889\n",
            "Epoch 29/50\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0892 - val_loss: 0.0884\n",
            "Epoch 30/50\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.0889 - val_loss: 0.0881\n",
            "Epoch 31/50\n",
            "235/235 [==============================] - 5s 23ms/step - loss: 0.0886 - val_loss: 0.0880\n",
            "Epoch 32/50\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.0884 - val_loss: 0.0876\n",
            "Epoch 33/50\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.0881 - val_loss: 0.0876\n",
            "Epoch 34/50\n",
            "235/235 [==============================] - 5s 23ms/step - loss: 0.0879 - val_loss: 0.0873\n",
            "Epoch 35/50\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.0877 - val_loss: 0.0868\n",
            "Epoch 36/50\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.0874 - val_loss: 0.0868\n",
            "Epoch 37/50\n",
            "235/235 [==============================] - 5s 23ms/step - loss: 0.0872 - val_loss: 0.0867\n",
            "Epoch 38/50\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.0870 - val_loss: 0.0866\n",
            "Epoch 39/50\n",
            "235/235 [==============================] - 5s 19ms/step - loss: 0.0868 - val_loss: 0.0859\n",
            "Epoch 40/50\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.0865 - val_loss: 0.0858\n",
            "Epoch 41/50\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.0863 - val_loss: 0.0858\n",
            "Epoch 42/50\n",
            "235/235 [==============================] - 5s 23ms/step - loss: 0.0861 - val_loss: 0.0853\n",
            "Epoch 43/50\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.0859 - val_loss: 0.0858\n",
            "Epoch 44/50\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.0858 - val_loss: 0.0851\n",
            "Epoch 45/50\n",
            "235/235 [==============================] - 5s 23ms/step - loss: 0.0856 - val_loss: 0.0848\n",
            "Epoch 46/50\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.0854 - val_loss: 0.0847\n",
            "Epoch 47/50\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.0852 - val_loss: 0.0846\n",
            "Epoch 48/50\n",
            "235/235 [==============================] - 5s 23ms/step - loss: 0.0851 - val_loss: 0.0846\n",
            "Epoch 49/50\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.0850 - val_loss: 0.0843\n",
            "Epoch 50/50\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.0848 - val_loss: 0.0843\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb145aa46a0>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualising the original image and reconstructed image**"
      ],
      "metadata": {
        "id": "d6n_dHhtvAyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get the reconstructed images\n",
        "reconstructed_imgs = model.predict(x_test)\n",
        "\n",
        "# Choose one image to visualize\n",
        "img_index = 1\n",
        "\n",
        "# Reshape the input and reconstructed images to their original shape\n",
        "input_img = x_test[img_index].reshape(28, 28)\n",
        "reconstructed_img = reconstructed_imgs[img_index].reshape(28, 28)\n",
        "\n",
        "# Plot the input and reconstructed images side by side\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "ax[0].imshow(input_img, cmap='gray')\n",
        "ax[0].set_title('Input Image')\n",
        "ax[1].imshow(reconstructed_img, cmap='gray')\n",
        "ax[1].set_title('Reconstructed Image')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nPl0U14iucLA",
        "outputId": "7937e0b4-af6c-4d51-b5a7-2f164bef3473",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Obtaining the reduced dimensions:**\n",
        "\n",
        "The reduced dimension of the image is obtained from the encoder model, which maps the input image to a lower-dimensional latent space. To access the reduced dimension, you can use the predict() method of the encoder model on your input data. \n",
        "\n"
      ],
      "metadata": {
        "id": "y6h2XVqowjEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the encoder model from the autoencoder\n",
        "encoder = Model(inputs=model.input, outputs=model.get_layer('encoder_output').output)\n",
        "\n",
        "# Get the reduced dimension of the input images\n",
        "encoded_imgs = encoder.predict(x_test)\n",
        "\n",
        "# Choose one image to visualize\n",
        "img_index = 1\n",
        "\n",
        "# Reshape the input and reconstructed images to their original shape\n",
        "print(x_test[img_index].shape)\n",
        "print(encoded_imgs[img_index].shape)"
      ],
      "metadata": {
        "id": "TiWc3jtdJlWt",
        "outputId": "ea92e3dd-02f4-4eb4-b8b8-98f69cced524",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step\n",
            "(784,)\n",
            "(32,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that the flattened data of 784 is reduced to 32 here!! "
      ],
      "metadata": {
        "id": "09iANVXWJlC6"
      }
    }
  ]
}